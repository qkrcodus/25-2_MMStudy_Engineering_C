- 웹 크롤러: 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 목적
  - 검색 엔진 인덱싱
  - 웹 아카이빙
  - 웹 마이닝
  - 웹 모니터링
 
## 1단계: 문제 이해 및 설계 범위 확정
- 웹 크롤러의 기본 알고리즘
  1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지 다운로드
  2. 다운받은 웹 페이지에서 URL 목록들을 추출
  3. 추출된 URL들을 다운로드 할 목록에 추가하고 위의 과정을 처음부터 반복
- 크롤러의 주된 용도: 검색 엔진 인덱싱
- 매달 얼마나 많은 웹 페이지 수집: 10억 개
- 새로 만들어진 웹 페이지나 수정된 웹 페이지 고려: O
- 수집한 웹 페이지 저장: 5년간
- 중복된 콘텐츠 페이지는 무시해도 됨

### 좋은 웹 크롤러가 지켜야 할 속성
- 규모 확장성: 병행성 활용
- 안정성: 비정상적 입력이나 환경에 잘 대응
- 예절: 짧은 시간 동안 너무 많은 요청을 보내서는 안 된다
- 확장성: 새로운 형태의 콘텐츠 지원이 쉬워야

### 개략적 규모 추정
- 10억 개의 웹페이지
- QPS = 10억/30일/24시간/3600초 = 대락 400페이지/초
- 최대 QPS = 2 * QPS = 800
- 웹페이지 크기 평균은 500k 가정
- 10억 페이지 * 500k = 500TB/월
- 5년치 데이터 보관 = 500TB * 12 * 5 = 30PB

### 개략적 설계안 제시 및 동의 구하기
<img width="678" height="506" alt="image" src="https://github.com/user-attachments/assets/c384f541-a003-45b1-941a-726e91e2482f" />

- 시작 URL 집합: 웹 크롤러가 크롤링을 시작하는 출발점 
- 미수집 URL 저장소: 다운로드 할 URL을 저장/관리하는 컴포넌트, FIFO 큐
- HTML 다운로더: 인터넷에서 웹 페이지를 다운로드하는 컴포넌트
- 도메인 이름 변환기: URL을 IP 주소로 변환하는 절
- 콘텐츠 파서: 웹페이지 다운로드 이후에는 파싱과 검증 절차를 거져야 함
- 중복 콘텐츠인가?: 웹페이지의 해시 값을 비교
- 콘텐츠 저장소: HTML 문서를 보관하는 시스템
- URL 추출기: HTML 페이지들을 파싱하여 링크를 골라내는 역할
- URL 필터: 크롤링 대상에서 특정 링크를 배제하는 역할
- 이미 방문한 URL?: 블룸필터, 해시 테이블 사용
- URL 저장소: 이미 방문한 URL을 보관

## 3단계: 상세 설계
- DFS vs. BFS
  - BFS 사용
  - FIFO 큐 사용 알고리즘
  - 예의 없는 크롤러
  - URL간 우선순위를 두기
- 미수집 URL 저장소
  - impolite: 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 보내는 것
  - 동일 웹사이트에 대해서는 한번에 한 페이지만 요청
  - 각 다운로드 스레드는 별도의 큐를 가지고 있어서 해당 큐에서 꺼난 URL만 다운로드
  - <img width="625" height="585" alt="image" src="https://github.com/user-attachments/assets/a32341c2-052d-4d8a-b50f-5c3a0ce1a3fc" />
- HTML 다운로더
  - 로봇 제외 프로토콜(Robots.txt)
  - 웹사이트가 크롤러와 소통하는 표준적 방법, 크롤러가 수집해도 되는 페이지 목록이 들어 있다
  - 성능 최적화 기법: 분산 크롤링, 도메인 이름 변환 결과 캐시, 지역성, 짧은 타임아웃
- 안정성 확보 전략: 안정 해시, 크롤링 상태 및 수집 데이터 저장, 예외 처리, 데이터 검증
- 확장성 확보 전략: 새로운 모듈을 끼워 넣음으로써 새로운 형태의 콘텐츠 지원 가능
- 문제 있는 콘텐츠 감지 및 회피 전략
  - 중복 콘텐츠: 해시나 체크섬
  - 거미 덫: url의 최대 길이를 제한하여 회피
  - 데이터 노이즈
 
## 4단계: 마무리
- 서버 측 렌더링
- 원치 않는 페이지 필터링
- 데이터베이스 다중화 및 샤딩
- 수평적 규모 확장성
- 가용성, 일관성, 안정성
- 데이터 분석 솔루션
