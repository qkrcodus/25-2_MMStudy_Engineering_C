## 7장 분산 시스템을 위한 유일 ID 생성기 설계

**1단계: 문제 이해 및 설계 범위 확정**

- **ID 특성:**
    - 유일성
    - 숫자로만 구성
    - 64비트로 표현 가능해야 함
    - 발급 날짜에 따라 정렬 가능해야 함
    - 초당 10,000개의 ID를 생성할 수 있어야 함

**2단계: 개략적 설계안 제시 및 동의 구하기**

분산 시스템에서 유일성이 보장되는 ID 만드는 방법

- **다중 마스터 복제:** 데이터베이스의 `auto_increment` 기능을 활용하여 k씩 증가시키는 방식
    - **장점:** 규모 확장성이 어느 정도 가능함
    - **단점:**
        - 여러 데이터센터에 걸쳐 규모를 늘리기 어려움
        - 시간이 흐름에 맞춰 커지도록 보장하기 어려움
        - 서버 추가/삭제 시 복잡함
- **UUID (Universally Unique Identifier):**128비트 길이로 각 웹 서버의 ID생성기가 독립적으로 생성하는 방식
    - ex) `09c93e62-50b4-468d-bf8a-c07el040bfl`
    - **장점:**
        - 단순하게 생성 가능, 서버 간 조율할 필요 없어 동기화 이슈 X
        - 각 서버가 각자 쓸 ID를 알아서 만드는 구조 → 규모 확장이 용이
    - **단점:**
        - 128비트로 긺(이 책에서의 요구사항은 64비트)
        - 시간 순 정렬이 불가능
        - 숫자 외의 값 포함 가능성

- **티켓 서버:** `auto_increment` 기능을 갖춘 데이터베이스 서버를 **중앙 집중형**으로 **하나만** 사용
    - **장점:**
        - 유일하고 숫자만으로 구성된 ID를 쉽게 생성 가능
        - 구현이 간단 → 중소 규모 애플리케이션에 적합
    - **단점:**
        - SPOF(Single-Point-of-Failure) 문제(중앙 집중 서버 다운 → 모든 시스템 마비)
        - 이를 회피하기 위한 서버 다중화 시 동기화 문제가 발생 가능

- **트위터 스노플레이크(Twitter Snowflake):**
    
    모든 요구사항을 만족시키는 독창적인 ID 생성 기법
    
    64비트 ID를 여러 섹션으로 분할하여 구성
    

![스크린샷 2025-08-18 오전 3.03.53.png](attachment:9f3f0c13-c830-4d21-aa01-99c463e5c10e:스크린샷_2025-08-18_오전_3.03.53.png)

- **ID 구조 (64비트):**
    - **사인(Sign) 비트 (1비트):**
        - 음수/양수 구별 등에 대비
    - **타임스탬프 (41비트):**
        - 기원 시각(Epoch) 이후 경과된 밀리초를 나타냄
        - 시간 순 정렬을 가능하게 함
    - **데이터센터 ID (5비트):**
        - 최대 32개의 데이터센터를 지원 가능
        - 시스템 시작 시 고정됨
    - **서버 ID (5비트):**
        - 데이터센터당 최대 32개의 서버를 지원 가능
        - 시스템 시작 시 고정됨
    - **일련번호 (12비트):**
        - 각 서버에서 ID 생성 시 1씩 증가
        - 1밀리초 경과할 때마다 0으로 초기화
        - 동일 밀리초 내에 여러 ID 생성 시 유일성 보장

**3단계: 상세 설계**

- **타임스탬프 (41비트):**
    - ID의 시간 순 정렬을 보장하는 핵심 요소
    - 시간이 흐름에 따라 점점 큰 값을 갖게 됨
    - 트위터 스노플레이크는 2010년 11월 4일 01:42:54 UTC를 기원 시각으로 사용
    - 41비트로 표현할 수 있는 값으로, 약 69년의 기간을 커버할 수 있음
    - 69년이 지나면 기원 시각을 바꾸거나, ID 체계를 다른 것으로 이전해야 함
- **데이터센터 ID 및 서버 ID (각 5비트):**
    - 시스템 시작 시 결정되어 고정됨
    - 변경 시 ID 충돌이 발생할 수 있으므로 주의 필요
- **일련번호 (12비트):** 1밀리초 내에 최대 2^12 = 4096개의 ID 생성을 구분(동일 밀리초 내에 유일성 보장 가능)

**4단계: 마무리**

- **시계 동기화 (Clock Synchronization):**
    - ID 생성 서버들이 동일한 시계를 사용해야 한다는 가정이 중요
    - NTP(Network Time Protocol) 등을 통해 실제 환경에서 시계 동기화를 구현해야 함
- **각 섹션 길이 최적화:**
    - 애플리케이션의 동시성 수준이나 수명에 따라 타임스탬프나 일련번호의 비트 길이를 조정
    - → 효율성을 높일 수 있음
- **고가용성 (High Availability):**
    - ID 생성기는 필수적인 컴포넌트 → 아주 높은 가용성을 제공해야 함

## 9장 웹 크롤러 설계

**1단계 문제 이해 및 설계 범위 확정**

- 웹 크롤러의 기본 알고리즘
    - URL 집합을 입력 → 해당 URL들이 가리키는 **모든 웹 페이지를 다운로드**
    - 다운받은 웹 페이지에서 **URL들을 추출**
    - 추출된 URL들을 **다운로드할** **URL 목록에 추가**
    - 위 과정을 처음부터 반복한다.
    
- 웹 크롤러가 만족시켜야 할 속성
    - 규모 확장성:
        - 오늘날 웹에는 수십억 개의 페이지가 존재함
        - 따라서 병행성(parallelism)을 활용하면 보다 효과적으로 웹 크롤링을 할 수 있음
    - 안정성(robustness):
        - 웹에는 잘못 작성된 HTML, 아무 반응이 없는 서버, 장애 및 악성 코드가 붙어 있는 링크 등이 있을 수 있음
        - 이런 비정상적 입력이나 환경에 잘 대응할 수 있어야 함
    - 예절(politeness):
        - 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안 됨
    - 확장성(extensibility):
        - 새로운 형태의 콘텐츠를 지원하기가 쉬워야 함
        - 예를 들어, 이미지 파일도 크롤링하고 싶다고 할 때, 이를 위해 전체 시스템을 새로 설계하는 것은 비효율적

---

**2단계 개략적 설계안 제시 및 동의 구하기**

- **시작 URL 집합**:
    - 크롤러가 크롤링을 시작하는 출발점
    - 크롤러가 가능한 한 많은 링크를 탐색할 수 있도록 하는 URL을 고르는 것이 바람직함
    - 일반적으로는 전체 URL 공간을 작은 부분집합으로 나누는 전략을 사용(주제 별로 나눔)
- **미수집 URL 저장소 (URL frontier)**:
    - 대부분 현대적 웹 크롤러는 크롤링 상태를
    - (1) 다운로드할 URL, (2)다운로드된 URL의 두 가지로 나눠 관리
    - 다운로드할 URL을 관리하는 FIFO 큐라고 생각하면 됨
- **HTML 다운로더**:
    - 인터넷에서 웹 페이지를 다운로드
- **도메인 이름 변환기**:
    - URL을 IP 주소로 변환
- **콘텐츠 파서**:
    - 다운로드된 웹 페이지를 파싱하고 검증
    - 이상한 웹 사이트인지 파악하는 독립된 컴포넌트
- **중복 콘텐츠 확인**:
    - 연구 결과에 따르면, 현재 약 29%의 웹 페이지 컨텐츠는 중복
    - 따라서 같은 컨텐츠를 여러 번 저장하는 문제 발생할 수 있음
    - `웹 페이지의 해시 값을 비교하여 중복 콘텐츠를 식별`
    - 어떻게 해시 값을 비교?
- **콘텐츠 저장소**:
    - HTML 문서를 저장
    - 인기 콘텐츠는 메모리에 두어 접근 시간 줄임
    - 대부분의 콘텐츠는 디스크에 저장
- **URL 추출기**:
    - HTML페이지를 파싱하여, 페이지에서 링크를 골라내어 상대 경로 → 절대 경로로 변환
- **URL 필터**:
    - 특정 콘텐츠 타입, 오류 발생 URL, 접근 제외 목록 등을 필터링
- **이미 방문한 URL 확인**:
    - 블룸 필터나 해시 테이블을 사용 → URL 저장소에서 이미 처리한 적이 있는 URL인지 추적
- **URL 저장소**:
    - 이미 방문한 URL을 보관

- 웹 크롤러 작동 흐름

---

**3단계 상세 설계**

에서는 핵심 컴포넌트를 심도 있게 다루고 최적화 및 안정성 확보 전략을 제시합니다.

- **DFS vs BFS**:
    - 웹은 유향 그래프(Directed Graph)와 같음
    - 그래프 탐색에 사용되는 DFS와 BFS 중, 웹 크롤러는 그래프 크기 예측이 어려운 DFS 대신 BFS(너비 우선 탐색)를 주로 사용

- **미수집 URL 저장소의 개선**:
    - **예의(politeness)**:
        - 동일 웹 사이트에 대한 과도한 요청을 방지해야 함
        - 호스트별(각 웹사이트 별)로 별도 FIFO 큐를 유지 → 시간차를 두어 처리 → 해당 큐에서 꺼낸 URL만 다운로드
        - **큐 라우터(queue router):**
            - 크롤러가 수집할 URL들을 미수집 URL 저장소에 넣으면,
            - **큐 라우터**는 같은 호스트(웹사이트)에 속한 URL들이 항상 **같은 FIFO 큐**로 들어가도록 분배
        - **큐 선택기(queue selector):**
            - **큐 선택기**는 이 호스트별 FIFO 큐들을 순회하면서 각 큐에서 URL을 하나씩 꺼냄
        - **작업 스레드(worker thread)**
            - 큐 선택기로부터 URL을 받은 **작업 스레드**는 해당 URL을 순차적으로 다운로드 작업을 수행
            - 이때 작업들 사이에 **일정한 지연 시간(delay)을 둠**
        - → 이 과정에서 시간 차를 두어 처리되므로 예의를 지키게 됨
    
    - **우선순위**:
        - 같은 키워드를 가지더라도, 웹 페이지에 따라 중요도 다름
        - **순위 결정 장치**:
            - URL을 입력받아 우선순위를 계산
            - PageRank, 트래픽 양, 갱신 빈도 등을 기준으로 URL에 우선순위를 부여
            - 큐 우선 순위별로 하나씩 할당
        - **큐 선택기:**
            - 우선순위가 높은 큐에서 URL을 더 자주 꺼내도록 함
    
    - **신선도**:
        - 웹 페이지의 변경 이력과 우선순위를 활용 → 중요한 페이지를 더 자주 재수집
    
    - **지속성 저장장치**:
        - 수억 개의 URL을 효율적으로 관리하기 위해 대부분의 URL은 디스크에 저장
        - IO 비용 절감을 위해 메모리 버퍼에 큐를 둠
        - 버퍼에 있는 데이터는 주기적으로 디스크에 기록

- **HTML 다운로더**:
    - **Robots.txt**:
        - 크롤링 전 웹사이트가 크롤링을 허용하는 페이지 목록을 확인
        - 이 파일을 주기적으로 다운받아 캐시에 보관
    
    **성능 최적화**: 
    
    - 분산 크롤링
        - 크롤링 작업을 여러 서버에 분산하는 방법
        - URL을 작은 단위로 분할 → 여러 서버가 각 일부의 다운로드를 담당
    - 도메인 이름 변환 결과 캐시
        - 크롤러는 DNS 요청을 보내고 결과를 받을 때까지 다음 작업 진행 불가
        - DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관해 놓고 크론 잡(cron job) 등을 돌려 주기적으로 갱신함
    - 지역성 활용
        - 크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법
    - 짧은 타임아웃 설정
        - 웹 서버에 따라 응답시간이 다름
        - 느린 웹사이트에 대응하여 최대 얼마나 크롤러가 기다릴지 설정
    
    **안정성**: 
    
    - 안정 해시
    - 크롤링 상태 및 수집 데이터 지속적 저장
        - 장애 발생 시에 쉽게 복구할 수 있도록 지속적으로 저장
    - 예외 처리
        - 에러가 발생해도 전체 시스템이 중단되지 않고 처리될 수 있도록 해야함
    - 데이터 검증
    
    **확장성**: 
    
    - 새로운 모듈(예: PNG 다운로더, 웹 모니터)을 추가하여 새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 설계

- **문제 있는 콘텐츠 감지 및 회피**:
    - **중복 콘텐츠**:
        - 해시나 체크섬을 사용하여 중복을 탐지
    - **거미 덫 (spider trap)**:
        - 크롤러가 무한 루프에 빠지도록 의도적으로 설계된 페이지
        - URL 최대 길이 제한 → 무한 루프를 회피
    - **데이터 노이즈**:
        - 광고, 스크립트 코드, 스팸 URL 등 가치 없는 콘텐츠를 제외